#Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?

"""R-squared, also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a linear regression 
model. It determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared
shows how well the data fit the regression model.


R-squared = 1 - (SSR / SST)
SSR (Sum of Squared Residuals) represents the sum of the squared differences between the observed dependent variable values and the predicted values from the regression model.
SST (Total Sum of Squares) represents the sum of the squared differences between the observed dependent variable values and the mean of the dependent variable.


An R-squared value of 0 indicates that the model does not explain any of the variability in the dependent variable.
An R-squared value of 1 indicates that the model perfectly explains all the variability in the dependent variable
"""


#Q2. Define adjusted R-squared and explain how it differs from the regular R-squared
"""Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) in linear regression. It addresses the issue of model complexity by penalizing the addition of independent variables, providing a more reliable measure of the goodness of fit when comparing models with different numbers of independent variables.

Adjusted R-squared is calculated using the formula:

Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]

Where:

R-squared is the regular coefficient of determination.
n represents the sample size (the number of observations).
k represents the number of independent variables (predictors) in the model.


R-squared does not account for the complexity of the model, while adjusted R-squared penalizes the addition of unnecessary independent variables.
R-squared represents the proportion of variance explained in the dependent variable, while adjusted R-squared represents the proportion of variance
explained that is adjusted for the number of predictors in the model.
R-squared ranges from 0 to 1, while adjusted R-squared can be negative. A negative adjusted R-squared suggests that the model fits the data worse than
a model with no predictors, indicating that the model is not suitable for the given data.
"""




#Q3. When is it more appropriate to use adjusted R-squared?
"""Adjusted R-squared can help in the process of variable selection by evaluating the impact of adding or removing predictors from the model.
 When comparing multiple regression models with different numbers of independent variables, using adjusted R-squared allows for a fair and 
 reliable comparison. It becomes more reliable as the sample size increases. """


#Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?
"""
Root Mean Squared Error (RMSE):
RMSE is a widely used metric that calculates the square root of the average of the squared differences between the predicted and actual values. 
It provides a measure of the typical magnitude of the prediction error.
RMSE = sqrt(MSE)
To calculate RMSE:
1.Calculate the squared difference between each predicted value (ŷ) and its corresponding actual value (y).
2.Take the average of these squared differences.
3.Take the square root of the average to obtain the RMSE.


Mean Squared Error (MSE):
MSE calculates the average of the squared differences between the predicted and actual values. It measures the average squared error between the
predicted and actual values, providing a measure of the overall model accuracy.
MSE = (1/n) * Σ(y - ŷ)^2
To calculate MSE:
Calculate the difference between each predicted value (ŷ) and its corresponding actual value (y).
Square each difference.
Take the average of these squared differences.


Mean Absolute Error (MAE):
MAE calculates the average of the absolute differences between the predicted and actual values. It measures the average absolute difference
between the predicted and actual values, providing a measure of the average prediction error.
MAE = (1/n) * Σ|y - ŷ|
To calculate MAE:
Calculate the absolute difference between each predicted value (ŷ) and its corresponding actual value (y).
Take the average of these absolute differences.
"""




#Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.
"""
Advantages:
1.RMSE, MSE, and MAE provide straightforward and intuitive measures of prediction error.
2.These metrics are sensitive to the magnitude of errors, capturing both small and large deviations between predicted and actual values. 
3.RMSE, MSE, and MAE are applicable to a wide range of regression problems and can be used with various types of dependent variables
(continuous, discrete, etc.)."""



#Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?
"""Lasso regularization, also known as L1 regularization, is a technique used in regression analysis to add a penalty term based on the absolute
values of the regression coefficients.


Lasso regularization adds the sum of the absolute values of the coefficients (L1 norm) as the penalty term, while Ridge regularization adds the sum
of the squared values of the coefficients (L2 norm).
Lasso regularization has the property of performing feature selection by driving some coefficients to exactly zero, on the other hand, Ridge 
regilarization can shrink coefficients close to zero but does not generally result in exact zero values.: Lasso regularization is known to perform
automatic variable selection and is particularly useful in the presence of multicollinearity (high correlation) among the predictors. It can
effectively choose one variable from a group of highly correlated variables while setting the others to zero. Ridge regularization, in contrast, does
not provide automatic variable selection and tends to shrink all coefficients towards zero without eliminating any variables.



When feature selection is desired, and there is a need to identify the most important predictors or dealing with high-dimensional datasets where the 
number of predictors is large compared to the number of observations or there is a multicollinearity among the predictors then  Lasso regularization
is more appropriate to use."""



#Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.
"""Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by introducing a penalty
term that discourages the model from fitting the training data too closely or relying heavily on individual features."""


#Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.

"""1.Regularized linear models, like Ridge and Lasso regression, assume a linear relationship between the independent variables and the dependent
variable. If the true relationship is highly non-linear or involves complex interactions, linear models may not capture the underlying patterns
effectively, leading to suboptimal results.
2. Regularized linear models tend to shrink coefficients towards zero or eliminate less important features. While this feature selection can be 
beneficial, it can also result in models that are less interpretable. It becomes more challenging to interpret the effects of individual predictors 
when coefficients are penalized or set to zero.
3. Regularized linear models introduce hyperparameters that need to be tuned, such as the regularization parameter (λ) in Ridge regression or Lasso 
regression. Determining the optimal values for these hyperparameters can be challenging, and the model's performance can be sensitive to their 
selection
4.Regularized linear models can still be influenced by outliers. While the regularization term helps in reducing the impact of outliers, extreme 
outliers can still have a disproportionate effect on the model's performance.
"""


"""Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has
an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"""


"""In this scenario, Model B with an MAE of 8 would be considered the better performer compared to Model A with an RMSE of 10. The reason is that MAE 
measures the average magnitude of the prediction errors without considering their squared values. Therefore, an MAE of 8 indicates that, on average, 
the predictions of Model B deviate from the actual values by 8 units. On the other hand, RMSE takes into account the squared differences between
predictions and actual values, amplifying the effect of larger errors. Hence, an RMSE of 10 implies that the average deviation of Model A's
predictions is greater than Model B's MAE of 8.


 the choice between RMSE and MAE can also depend on the nature of the dependent variable and the associated costs or consequences of prediction errors.
 For example, if the variable represents a financial measure, such as house prices, a metric like RMSE that penalizes larger errors more heavily may 
 be more appropriate. However, if the variable represents a medical measure, where overestimating or underestimating values has different implications,
 MAE might be more suitable."""



"""Q10.You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization 
with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose 
as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"""


"""To determine the better performer, several factors should be considered:
1.The choice of the regularization parameter (λ) also influences the model's performance. The given parameters, 0.1 for Ridge and 0.5 for Lasso, 
might need to be fine-tuned through techniques like cross-validation to identify the optimal values that yield the best results for each model.
2.If feature selection is desirable or if there is a large number of predictors, Lasso regularization may be preferred. 
3.f interpretability is a priority, Ridge regularization might be preferred as it tends to retain all the features but shrink their coefficients. 
"""


