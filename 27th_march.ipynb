{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8a0b9-405d-4569-ac30-fa3fadb4d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\"\"\"Simple linear regression is a regression technique that models the relationship between a dependent variable and a single independent variable.\n",
    "It assumes a linear relationship between the variables, allowing us to predict the value of the dependent variable based on the value of the \n",
    "independent variable.\n",
    "Suppose we want to examine the relationship between the number of hours studied (independent variable) and the exam score (dependent variable) of \n",
    "students. We collect data from 50 students, recording the number of hours they studied and their corresponding exam scores. We can use simple linear\n",
    "regression to model how the number of hours studied predicts the exam score.\n",
    "\n",
    "\n",
    "Multiple linear regression is a regression technique that models the relationship between a dependent variable and multiple independent variables. It \n",
    "allows us to examine how multiple factors simultaneously influence the dependent variable, taking into account their combined effects.\n",
    "Let's consider a real estate scenario where we want to predict the price of a house (dependent variable) based on various factors such as the size of\n",
    "the house (in square feet), the number of bedrooms, and the distance to the city center (independent variables). We collect data on several houses, \n",
    "recording the size, number of bedrooms, distance, and their corresponding prices. Multiple linear regression allows us to build a model that \n",
    "incorporates all these variables to predict house prices.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336898e-b2df-459f-ae09-974538259732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\"\"\"\n",
    "1.The relationship between the dependent variable and the independent variables should be linear.\n",
    "2.The observations should be independent of each other. Independence assumes that there is no correlation or dependency between the residuals.\n",
    "3.Linear regression assumes that the residuals (i.e., the differences between observed and predicted values) follow a normal distribution.\n",
    "4.Outliers are extreme observations that have a significant influence on the regression model. They can distort the regression line and affect the \n",
    "coefficient estimates. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972f200-e40c-49cf-a1c7-266ac4676d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\"\"\"\n",
    "In linear regression, the slope and intercept represent the parameters of the model that describe the relationship between the independent variable\n",
    "and the dependent variable. \n",
    "Here's how you can interpret the slope and intercept in a linear regression model:\n",
    "Intercept: The intercept represents the predicted value of the dependent variable when all independent variables are zero. It indicates the starting \n",
    "point or the value of the dependent variable when the independent variables have no impact. The intercept can have a meaningful interpretation \n",
    "depending on the context of the problem. It is important to note that the interpretation of the intercept is contingent on the scaling and measurement\n",
    "units of the variables.\n",
    "Example: we want to predict a person's weight (dependent variable) based on their height (independent variable). In this case, the intercept would\n",
    "represent the estimated weight when the person's height is zero, which does not have a practical interpretation.\n",
    "\n",
    "Slope: The slope represents the change in the dependent variable associated with a one-unit increase in the corresponding independent variable,\n",
    "holding other variables constant. It indicates the magnitude and direction of the relationship between the independent variable(s) and the dependent \n",
    "variable.\n",
    "Example: Continuing with the weight prediction scenario, suppose we build a linear regression model using height as the independent variable. If the \n",
    "slope is estimated as 2.5, it means that, on average, for every one-unit increase in height (e.g., one inch or one centimeter), we expect the person's\n",
    "weight to increase by 2.5 units (e.g., pounds or kilograms), assuming all other factors remain constant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905efe46-74a5-4f82-9cd7-14da1d61217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\"\"\"Gradient descent is an optimization algorithm commonly used in machine learning to find the minimum of a function. It is particularly effective in \n",
    "training machine learning models, such as neural networks, by iteratively adjusting the model's parameters to minimize a given cost or loss function.\n",
    "STEPS:\n",
    "1.Initialization: We start by initializing the model's parameters with some initial values.\n",
    "\n",
    "Forward Pass: We use the current parameter values to make predictions on the training data and calculate the loss function, which quantifies the\n",
    "difference between the predicted and actual values.\n",
    "\n",
    "2.Gradient Calculation: The next step is to compute the gradients of the loss function with respect to each parameter. This involves calculating the\n",
    "partial derivatives of the loss function with respect to each parameter.\n",
    "\n",
    "3.Parameter Update: With the gradients calculated, we update the model's parameters by taking a small step in the direction of the negative gradient. \n",
    "The step size is determined by the learning rate, which controls the magnitude of the parameter update.\n",
    "\n",
    "4.Iteration: Steps 2 to 4 are repeated for a certain number of iterations or until a stopping criterion is met. The aim is to iteratively adjust the \n",
    "parameters in the direction that reduces the loss function.\n",
    "\n",
    "5.Convergence: The process continues until convergence, where the parameters reach a point where further updates do not significantly improve the \n",
    "model's performance or until a predefined maximum number of iterations is reached.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c1c2e-937d-4e80-9fd6-c402318c6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\"\"\"Multiple linear regression is an extension of simple linear regression that allows for the prediction of a dependent variable based on multiple \n",
    "independent variables.\n",
    "\n",
    "DIFFERENCE:\n",
    "1.In simple linear regression, only one independent variable is used to predict the dependent variable. In contrast, multiple \n",
    "linear regression incorporates two or more independent variables to model the relationship with the dependent variable.\n",
    "2. In simple linear regression, the coefficient represents the change in the dependent variable for a unit change in the independent variable. In \n",
    "multiple linear regression, the interpretation of coefficients becomes more nuanced. Each coefficient represents the change in the dependent variable \n",
    "while holding all other independent variables constant.\n",
    "3.In simple linear regression, the equation has a straightforward form with a single independent variable and a slope-intercept form. In multiple \n",
    "linear regression, the equation becomes more complex with multiple independent variables and multiple coefficients.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b5e70-c21d-4a34-95c7-9bae78c37d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\"\"\"Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other\n",
    "\n",
    "Detection multicollinearity:\n",
    "1.Correlation Matrix: Calculate the correlation coefficients between each pair of independent variables. High correlation coefficients \n",
    "(close to +1 or -1) indicate potential multicollinearity.\n",
    "2.Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures the extent to which the variance of the estimated \n",
    "regression coefficient is increased due to multicollinearity. A VIF value greater than 1 indicates potential multicollinearity, with higher values \n",
    "suggesting a stronger correlation between variables.\n",
    "\n",
    "Addressing  multicollinearity:\n",
    "1. Identify and remove one or more of the highly correlated independent variables from the model. By removing redundant variables, you can reduce \n",
    "multicollinearity. \n",
    "2.If feasible, collect more data to increase the sample size. A larger sample size can help mitigate the impact of multicollinearity.\n",
    "3.Regularization techniques that introduce a penalty term to the regression model. They help in shrinking the coefficients of the correlated variables,\n",
    "effectively reducing the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273fe54-83fa-4d58-a373-532b09740db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\"\"\"Polynomial regression is a type of regression analysis that allows for modeling the relationship between the independent variable(s) and the\n",
    "dependent variable using a polynomial function. \n",
    "\n",
    "1.Linear regression assumes a linear relationship between the independent and dependent variables, represented by a straight line. Polynomial\n",
    "regression allows for non-linear relationships by introducing polynomial terms, resulting in curves or more complex shapes.\n",
    "2.Polynomial regression provides more flexibility in modeling complex relationships between variables.\n",
    "3. Polynomial regression models with high polynomial degrees (large n) can overfit the data.\n",
    "4. Interpreting the coefficients in polynomial regression becomes more complex as the number of polynomial terms increases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651a587-5db7-4e23-ae73-fbaef54879ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use\n",
    "#polynomial regression?\n",
    "\n",
    "\"\"\"\n",
    "Advantages:\n",
    "1. Polynomial regression can capture non-linear relationships between variables, allowing for more flexible modeling.\n",
    "2.When the relationship between the variables is non-linear, polynomial regression can provide better accuracy and fit to the data compared to \n",
    "linear regression.\n",
    "3.Polynomial regression allows for the interpretation of higher-order polynomial terms.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "1. Polynomial regression models with high polynomial degrees (large n) can easily overfit the data\n",
    "2.Choosing the optimal degree of the polynomial is crucial in polynomial regression. Selecting an inappropriate degree can lead to underfitting or\n",
    "overfitting. \n",
    "3. With the introduction of polynomial terms, the model becomes more complex, both in terms of interpretation and computation. \n",
    "\n",
    "\n",
    "When there is a non-linear relationship between the variables, dataset is relatively small to medium-sized then polynomial regression is a \n",
    "suitable choice.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
