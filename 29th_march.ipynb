{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded60a1e-4149-4870-b95e-c70de8a4bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "'''The main objective of Lasso regression is to minimize the sum of squared residuals while simultaneously adding a penalty term to the cost function.\n",
    "This penalty term is the sum of the absolute values of the coefficients multiplied by a regularization parameter (λ). The regularization parameter\n",
    "controls the strength of the penalty and helps to determine the amount of shrinkage applied to the coefficients.\n",
    "\n",
    "\n",
    "Compared to other regression techniques Lasso regression has some distinct differences like it performs automatic feature selection by shrinking some\n",
    "coefficients to zero, Lasso regression can introduce some bias due to the penalty term, but it can help reduce the variance of the model. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811bf07-36fa-4f49-872b-84e1009b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\"\"\"Lasso regression simplifies the model by reducing the number of features included in the final model.\n",
    "By reducing the number of features, Lasso regression helps to combat overfitting, which occurs when a model is too complex and fits the training data \n",
    "too closely.\n",
    "By selecting the most important features and shrinking the coefficients of less important features, Lasso regression can enhance the model's ability\n",
    "to generalize well to unseen data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66083e38-d54e-4823-8b1e-ecffa3c94172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\"\"\"The sign of the coefficient(positive or negative)indicates the direction of the relationship between the predictor variable and the target variable.\n",
    "A positive coefficient suggests a positive correlation, meaning that an increase in the predictor variable leads to an increase in the target variable\n",
    ", while a negative coefficient indicates a negative correlation, meaning that an increase in the predictor variable leads to a decrease in the target\n",
    "variable.\n",
    "The magnitude of the coefficient represents the strength of the relationship between the predictor variable and the target variable. A larger \n",
    "coefficient indicates a stronger influence, while a smaller coefficient suggests a weaker influence.\n",
    "In Lasso regression, the coefficients of irrelevant or less important features are set to zero. This means that if a coefficient is zero, the \n",
    "corresponding predictor variable does not contribute to the model's prediction. Variables with non-zero coefficients are considered important for\n",
    "the model's prediction.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f557c-47e2-4c8f-b73b-d7ea27cb631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\"\"\"\n",
    "1.Regularization parameter (λ or alpha): This parameter controls the strength of the penalty term in Lasso regression. It determines the amount of \n",
    "shrinkage applied to the coefficients. A higher value of λ or alpha results in stronger regularization, which increases the amount of shrinkage and \n",
    "leads to more coefficients being pushed towards zero. Conversely, a lower value of λ or alpha reduces the amount of shrinkage, allowing more\n",
    "coefficients to remain non-zero.\n",
    "\n",
    "2.Max iterations or convergence threshold: Increasing the max iterations allows the algorithm to run for a longer time, potentially improving the \n",
    "convergence.Similarly, setting a lower convergence threshold makes the algorithm more sensitive to small changes in the coefficients, potentially \n",
    "leading to a longer runtime.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6826a9c-2e5f-4054-a06c-43457cfbecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\"\"\"Yes,\n",
    "1. One way to incorporate non-linearity into Lasso regression is by introducing polynomial features. By creating new features that are non-linear \n",
    "transformations of the original features (e.g., squaring the predictors), you can capture non-linear relationships between the predictors and the\n",
    "target variable. \n",
    "2. Instead of using polynomial features, you can use non-linear basis functions to model non-linear relationships. Basis functions are functions that\n",
    "transform the original predictors into a new space where the relationship with the target variable becomes linear or more manageable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ae90c-2532-4369-b7db-11c949153568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\"\"\"\n",
    "1.Ridge regression adds a penalty term to the cost function based on the sum of squared coefficients multiplied by a regularization parameter (λ). \n",
    "This penalty term is known as L2 regularization. On the other hand, Lasso regression adds a penalty term based on the sum of the absolute values of\n",
    "the coefficients multiplied by a regularization parameter (λ), which is referred to as L1 regularization. The choice between L2 and L1 regularization\n",
    "affects how the coefficients are regularized.\n",
    "2.: In Ridge regression, the penalty term from the L2 regularization shrinks the coefficients towards zero, but it does not set them exactly to zero.\n",
    "This means that Ridge regression retains all features in the model, even if their coefficients are small. In contrast, Lasso regression's L1 \n",
    "regularization can shrink coefficients towards zero and set them exactly to zero. This property allows Lasso regression to perform feature selection \n",
    "and produce sparse models by automatically excluding irrelevant features.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98879e8-39d1-4c0f-a248-d9456396420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\"\"\"Yes, Lasso regression can handle multicollinearity in the input features.\n",
    "When there are highly correlated features, Lasso regression tends to select one feature from the correlated group and set the coefficients of the \n",
    "remaining correlated features to zero. By doing so, it effectively chooses the most important feature from the group and discards the redundant ones.\n",
    "This helps in reducing the impact of multicollinearity by excluding less important variables and providing a more interpretable model.\n",
    "Lasso regression's feature selection property indirectly addresses multicollinearity, it's important to note that the specific feature selection \n",
    "behavior depends on the data and the strength of the correlation. Lasso regression's ability to handle multicollinearity may vary depending on the\n",
    "extent of correlation and the size of the dataset.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca2be5-b9f0-44e2-a0c8-a6e3148b70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8.How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\"\"\"Cross-validation is a widely used technique for selecting the regularization parameter. It involves splitting the dataset into training and validation \n",
    "subsets. The model is trained on the training set using different λ values, and the performance is evaluated on the validation set. The λ value that\n",
    "yields the best performance metric (e.g., mean squared error or R-squared) on the validation set is selected as the optimal λ value. Common cross-\n",
    "validation methods include k-fold cross-validation and leave-one-out cross-validation.\n",
    "Grid search is a systematic approach that involves defining a grid of λ values and evaluating the model performance for each λ value in the grid. \n",
    "The performance metric is computed for each λ value using cross-validation or another evaluation method. The λ value that results in the best \n",
    "performance metric is selected as the optimal λ value. Grid search can be computationally expensive, especially with a large number of λ values, \n",
    "but it ensures a thorough exploration of the parameter space.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
