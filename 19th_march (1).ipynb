{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb49df-0cb5-45c5-8d5d-f2755ad61a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA - Principle Component Analysis, unsupervised ML algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4062063-78d3-44bf-a219-97608e8dee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "______________________________________________________________\n",
      "   sepal_length  sepal_width  petal_length   petal_width\n",
      "0      0.222222     0.625000      0.067797      0.041667\n",
      "1      0.166667     0.416667      0.067797      0.041667\n",
      "2      0.111111     0.500000      0.050847      0.041667\n",
      "3      0.083333     0.458333      0.084746      0.041667\n",
      "4      0.194444     0.666667      0.067797      0.041667\n"
     ]
    }
   ],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "'''Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range. It transforms the values of a \n",
    "feature into a new range, typically between 0 and 1. Min-Max scaling is especially useful when the input features have different scales, and you want to bring them to a \n",
    "common scale.\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "Original ages: [20, 30, 40, 50, 60]\n",
    "X_min = 20\n",
    "X_max = 60\n",
    "Scaled ages: [(20-20)/(60-20), (30-20)/(60-20), (40-20)/(60-20), (50-20)/(60-20), (60-20)/(60-20)]\n",
    "Scaled ages: [0, 0.1667, 0.3333, 0.5, 1]\n",
    "\n",
    "'''\n",
    "#example:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df = sns.load_dataset('iris')\n",
    "normalized_data = pd.DataFrame(min_max_scaler.fit_transform(df[['sepal_length','sepal_width','petal_length', 'petal_width']]),\n",
    "                               columns = ['sepal_length','sepal_width','petal_length',' petal_width'])\n",
    "print(df.head())\n",
    "print('______________________________________________________________')\n",
    "print(normalized_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41fb44a-2206-4ce0-9ae3-eb9606289722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0        0.803773     0.551609      0.220644     0.031521\n",
      "1        0.828133     0.507020      0.236609     0.033801\n",
      "2        0.805333     0.548312      0.222752     0.034269\n",
      "3        0.800030     0.539151      0.260879     0.034784\n",
      "4        0.790965     0.569495      0.221470     0.031639\n",
      "..            ...          ...           ...          ...\n",
      "145      0.721557     0.323085      0.560015     0.247699\n",
      "146      0.729654     0.289545      0.579090     0.220054\n",
      "147      0.716539     0.330710      0.573231     0.220474\n",
      "148      0.674671     0.369981      0.587616     0.250281\n",
      "149      0.690259     0.350979      0.596665     0.210588\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "'''The Unit Vector technique, also known as vector normalization or feature scaling by unit length, is a data preprocessing technique used to rescale numeric features to\n",
    "have a unit length or magnitude of 1. Unlike Min-Max scaling, which brings the values within a specific range, Unit Vector scaling focuses on the direction or orientation of\n",
    "the data points rather than their absolute values.\n",
    "X_scaled = X / ||X||\n",
    "Height: [170, 180, 160]\n",
    "Weight: [70, 80, 90]\n",
    "\n",
    "||[170, 70]|| = sqrt(170^2 + 70^2) = 181.0193\n",
    "||[180, 80]|| = sqrt(180^2 + 80^2) = 193.6492\n",
    "||[160, 90]|| = sqrt(160^2 + 90^2) = 181.8653\n",
    "\n",
    "Now, we can divide each data point by its respective magnitude:\n",
    "\n",
    "Height_scaled: [170 / 181.0193, 180 / 193.6492, 160 / 181.8653]\n",
    "Weight_scaled: [70 / 181.0193, 80 / 193.6492, 90 / 181.8653]\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "norm = pd.DataFrame(normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]),columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835b1dd-3d01-4434-8bcd-48862bc303b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "'''PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining the most\n",
    "important information. It achieves this by identifying the principal components, which are new uncorrelated variables that capture the maximum variance in the data.\n",
    "\n",
    "In this method a orthogonal line is drawn between the axis means that in the middle of the  data points a line is drawn and all the data points on both axis projected on \n",
    "that single line. Like this the dimensionality is reduced.\n",
    "\n",
    "Original dataset:\n",
    "Height: [170, 180, 160]\n",
    "Weight: [70, 80, 90]\n",
    "Age: [25, 30, 35]\n",
    "\n",
    "Standardize the data:\n",
    "Standardized height: [0, 1, -1]\n",
    "Standardized weight: [-1, 0, 1]\n",
    "Standardized age: [-1, 0, 1]\n",
    "\n",
    "Compute the covariance matrix:\n",
    "Covariance matrix:\n",
    "[[1, 0, 0],\n",
    "[0, 1, 0],\n",
    "[0, 0, 1]]\n",
    "\n",
    "Compute the eigenvectors and eigenvalues:\n",
    "Eigenvectors:\n",
    "[[1, 0, 0],\n",
    "[0, 1, 0],\n",
    "[0, 0, 1]]\n",
    "\n",
    "Eigenvalues: [1, 1, 1]\n",
    "\n",
    "Select the principal components:\n",
    "Since all the eigenvalues are equal, any combination of the three eigenvectors can be selected. Let's choose the first two eigenvectors.\n",
    "Selected eigenvectors:\n",
    "[[1, 0, 0],\n",
    "[0, 1, 0]]\n",
    "\n",
    "Project the data:\n",
    "Projected data:\n",
    "[[0, 1],\n",
    "[-1, 0],\n",
    "[-1, 0]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d83140-338f-475e-be9a-165bd35cc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "'''PCA (Principal Component Analysis) can be used for feature extraction, which is a process of creating new features from the existing ones. Feature extraction aims to \n",
    "transform the original features into a smaller set of features that still capture the essential information.\n",
    "\n",
    "In the context of PCA, feature extraction involves selecting the top-k principal components and using them as the new features. These principal components are the linear \n",
    "combinations of the original features that explain the most variance in the data.\n",
    "\n",
    "\n",
    "Original dataset:\n",
    "Age: [25, 30, 35, 40]\n",
    "Height: [170, 180, 160, 175]\n",
    "Weight: [70, 80, 90, 75]\n",
    "Income: [$50,000, $60,000, $70,000, $80,000]\n",
    "\n",
    "Standardize the data:\n",
    "Standardized age: [-1.3416, -0.4472, 0.4472, 1.3416]\n",
    "Standardized height: [-0.4472, 1.3416, -1.3416, 0.4472]\n",
    "Standardized weight: [-1.3416, -0.4472, 0.4472, 1.3416]\n",
    "Standardized income: [-1.3416, -0.4472, 0.4472, 1.3416]\n",
    "\n",
    "Compute the covariance matrix:\n",
    "Covariance matrix:\n",
    "[[1.3333, 1.3333, 1.3333, 1.3333],\n",
    "[1.3333, 1.3333, 1.3333, 1.3333],\n",
    "[1.3333, 1.3333, 1.3333, 1.3333],\n",
    "[1.3333, 1.3333, 1.3333, 1.3333]]\n",
    "\n",
    "Compute the eigenvectors and eigenvalues:\n",
    "Eigenvectors:\n",
    "[[ 0.5, 0.5, -0.5, -0.5],\n",
    "[ 0.5, -0.5, 0.5, -0.5]]\n",
    "\n",
    "Eigenvalues: [5.3333, 0]\n",
    "\n",
    "Select the principal components:\n",
    "Since one eigenvalue is 0, we only have one non-zero principal component. Let's select the first eigenvector.\n",
    "Selected eigenvector:\n",
    "[ 0.5, 0.5, -0.5, -0.5]\n",
    "\n",
    "Project the data:\n",
    "Projected data:\n",
    "[0.4472, 0.4472, -0.4472, -0.4472]\n",
    "After applying PCA, we have extracted the most important feature, which is a linear combination of the original features. This new feature represents the direction of \n",
    "maximum variance in the data and can be used for further analysis or modeling. By selecting fewer principal components, we effectively reduced the dimensionality of the\n",
    "dataset while preserving the most significant information.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0bca2-86e6-4fdc-96c0-5ea84e6b62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time.\n",
    "#Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "'''I will try to understand the dataset and the specific features it contains, such as price, rating, and delivery time. Determine the range and distribution of values for \n",
    "each feature.For each feature, identify the minimum and maximum values present in the dataset. This step allows to define the range to which the values will be scaled.\n",
    "Then the formula will be applied:X_scaled = (X - X_min) / (X_max - X_min)\n",
    "For each feature in the dataset, apply the Min-Max scaling formula to obtain the scaled values. This process will be for all the relevant features, such as price, rating, and\n",
    "delivery time.\n",
    "The scaled dataset can now be used in recommendation system. The scaled values ensure that all features are on a comparable scale, making it easier to analyze and model the\n",
    "data effectively.\n",
    "\n",
    "By applying Min-Max scaling, transform the feature values to a common range (0 to 1), preserving the relative relationships between the data points while removing the \n",
    "influence of the original scale. This normalization facilitates accurate comparisons and analysis of the features, enabling the recommendation system to consider\n",
    "all relevant factors equally.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46082b09-8012-4f2e-be9b-3b4ffc59c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how\n",
    "#you would use PCA to reduce the dimensionality of the dataset.\n",
    "'''\n",
    "Gain a thorough understanding of the dataset, including the various features it contains, such as company financial data and market trends.Assess the size of the dataset and \n",
    "the number of features available.\n",
    "Before applying PCA, it's essential to standardize the dataset. This step involves transforming the features to have zero mean and unit variance. Standardization ensures \n",
    "that features with larger scales do not dominate the PCA analysis.\n",
    "Calculate the covariance matrix of the standardized dataset. The covariance matrix measures the relationships between different features. It provides insights into the\n",
    "variance and covariance structure of the data.\n",
    "Perform eigendecomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues\n",
    "indicate the amount of variance explained by each principal component. Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "Decide on the number of principal components. This selection depends on the desired level of dimensionality reduction and the amount of variance  to retain in the dataset. \n",
    "Typically, he top-k eigenvectors that explain the most variance will choose. These eigenvectors become the principal components that will form the reduced-dimensional space.\n",
    "Multiply the standardized dataset by the selected eigenvectors to obtain the transformed data in the reduced-dimensional space. This projection maps the original dataset \n",
    "onto the new set of uncorrelated features (principal components).\n",
    "The reduced-dimensional dataset, consisting of the transformed features, can now be used in stock price prediction model. The dimensionality reduction achieved through\n",
    "PCA helps to eliminate redundant or less informative features, simplifies the dataset, and focuses on the most significant information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e149be56-de77-4438-95e0-5744e8bce965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = [1, 5, 10, 15, 20]\n",
    "df = pd.DataFrame(data)\n",
    "scaled_data = min_max.fit_transform(df)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e47ee1-449b-4a14-a199-39722e5c4bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance of PC1: 0.901370694173991\n",
      "Explained Variance of PC2: 0.09825111829674554\n",
      "Explained Variance of PC3: 0.00037818752926356786\n",
      "Explained Variance of PC4: 2.5918199527373645e-32\n",
      "Cumulative Explained Variance: 1.0000000000000002\n",
      "Number of Components to Retain: 2\n",
      "Reduced Feature Matrix:\n",
      "[[ 0.11631388 -0.11586161]\n",
      " [-1.90521341 -0.97856135]\n",
      " [ 2.71941058  0.16753009]\n",
      " [-2.2757703   0.9813193 ]\n",
      " [ 1.34525925 -0.05442644]]\n"
     ]
    }
   ],
   "source": [
    "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components \n",
    "#would you choose to retain, and why?\n",
    "\n",
    "'''\n",
    "When performing feature extraction using PCA on a dataset with features like height, weight, age, gender, and blood pressure, the number of principal components to retain\n",
    "depends on the desired level of dimensionality reduction and the amount of variance we want to preserve in the dataset.\n",
    "To determine the number of principal components to retain, we typically consider the cumulative explained variance ratio. This metric tells us the proportion of the total \n",
    "variance in the dataset that is explained by each principal component.Atfirst Calculate the covariance matrix of the dataset. Then find the eigenvalues and eigenvectors of\n",
    "the covariance matrix.Then Sort the eigenvalues in descending order. Compute the explained variance ratio for each principal component by dividing its eigenvalue by the \n",
    "sum of all eigenvalues. Calculate the cumulative sum of the explained variance ratios.\n",
    "Choose the number of principal components that explain a significant portion of the total variance in the dataset. A common rule of thumb is to retain the principal\n",
    "components that explain a cumulative variance of around 80% to 95%. However, the specific threshold may vary depending on the application and specific requirements.'''\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'height': [170, 165, 180, 155, 175],\n",
    "    'weight': [65, 60, 75, 50, 70],\n",
    "    'age': [30, 25, 35, 28, 32],\n",
    "    'gender': ['M', 'F', 'M', 'F', 'M'],\n",
    "    'blood_pressure': [120, 110, 130, 115, 125]\n",
    "})\n",
    "features = ['height', 'weight', 'age', 'blood_pressure']\n",
    "X = data[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "for i, explained_var in enumerate(explained_variance_ratio):\n",
    "    print(f\"Explained Variance of PC{i+1}: {explained_var}\")\n",
    "print(f\"Cumulative Explained Variance: {cumulative_explained_variance[-1]}\")\n",
    "\n",
    "num_components = np.sum(cumulative_explained_variance < 0.95) + 1\n",
    "print(f\"Number of Components to Retain: {num_components}\")\n",
    "\n",
    "X_reduced = X_pca[:, :num_components]\n",
    "\n",
    "print(\"Reduced Feature Matrix:\")\n",
    "print(X_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b23b34-5492-4aaa-a198-0f86a0027bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
